VM: Virtual Machine 虚拟机
VMM: VM Monitor 虚机监控台，类似Hypervisor，用户空间和内核空间都有VMM
Host: 宿主机
Guest: 客户机
KVM： Kernel-based VM
KVM is a full virtualization solution for Linux on x86 hardware containing virtualization extensions (Intel VT or AMD-V).
KVM将Intel VT-x(x86)上的硬件辅助虚拟化成果集成到Linux内核态中，用户态在QEMU(kvmtool)

虚拟化的3个条件：等价性、高效性、资源控制

QEMU是陷入和模拟模型，满足等价性、资源控制，但不满足高效性（是模拟）

特权级压缩：Guest的用户指令运行在Host的用户模式，但Guest的特权指令也运行在Host的用户模式，这时处理器会触发异常，陷入VMM，由VMM执行特权指令
但并不是所有Guest的特权指令，都需要陷入VMM执行

敏感指令包括：特权指令(运行在ring0)和非特权的敏感指令(运行在ring1-3)

ring0-ring3
ring0:kernel
ring1-2:device driver
ring3:user

x86架构虚拟化的障碍：Guest的非特权敏感指令运行在ring1-3，x86 Host不会将这些指令交给VMM处理
解决方案：
1、动态翻译指令，如果是非特权敏感指令，就交给VMM处理
2、Intel VT技术(虚拟化扩展技术)，VMX root mode和VMX non-root mode，VM_entry和VM_exit，让Guest的kernel运行在Guest的ring0中，不会被特权级压缩到Host的用户模式

VMX：基于处理器硬件实现的虚拟化
VM_entry：从Host进入Guest
VM_exit：从Guest返回Host
VMCS：保存VM_entry和VM_exit过程中的上下文

VCPU是Host虚拟出来的cpu处理器
VM是Host上的进程，VCPU是同一个VM里的线程

在Guest里，感知不到cpuid信息，只知道cpu型号，因此在Guest里的核间中断，要切换到Host的VMM处理

访问外设IO的方式：PIO(programmed IO)、MMIO(memory-mapped IO)
MMIO：将IO映射到内存地址空间，而非IO地址空间，cpu只需要访问映射的内存，即可以访问IO
PIO：cpu通过in\out\ins\outs指令直接访问IO

Guest访问MMIO，产生缺页异常，回到Host的KVM，KVM无法映射出内存页，就只能模拟Guest访问MMIO过程

APIC：Advanced Programmable Interrupt Controller 可编程中断控制器
LAPIC：Local APIC 处理器特有的APIC，多用于SMP的核间中断
Intel支持一种专门的虚拟机退出原因：apic access

cpuid指令、hlt指令是敏感指令，需要Host的KVM执行

KVM模拟cpuid指令

KVM模拟hlt指令


嵌入式弹性底座
弹性Elastic：Hypervisor把物理硬件抽象成vcpu、vmem、vgic、virtio，OS作为虚拟机运行在它们之上。Hypervisor弹性地部署OS，弹性体现在分时共享硬件资源，提高资源利用率。

1、高效地混合部署
2、高效地通信与协作
3、高效地隔离与保护
4、高效的资源共享与调度

轻量级容器虚拟化
Cgroups：谷歌发布的进程容器(Process Containers)，提供一系列进程级资源隔离技术，包括cpu、内存、IO
LXC(Linux Containers)：Linux推出的Cgroups+Namespace，Cgroups按照进程组的维度隔离资源，并用Namespace标识进程组。容器Containers只支持运行在同一个宿主操作系统上，不能跨操作系统。
Docker：基于LXC技术，Docker公司推出开源容器引擎Docker，工作方式：只读的静态镜像+可写的动态容器，并提供一套创建、注册、部署、管理的容器工具集。
K8S(Kubernetes)：基于Docker，谷歌推出容器的集群管理平台K8S，自动化管理容器，解决大规模容器的编排、管理、调度问题。后续演化：Faas、PaaS等等。

虚拟化
硬件虚拟化：QEMU，虚拟硬件包括vcpu、vmem、vgic、virtio等，QEMU自身支持纯软件模拟（非虚拟化），结合KVM可以实现对硬件访问的全虚拟化。
操作系统级虚拟化（系统虚拟化）：虚拟机，Libvirt接口支持创建、删除、暂停、迁移虚拟机。
进程级虚拟化：容器
应用程序级虚拟化：沙箱

在虚拟化中，Hypervisor向下负责管理物理硬件资源（硬件虚拟化），向上完成Guest虚拟机的敏感指令请求（系统虚拟化）。
KVM/QEMU就是一个Hypervisor方案





以Zephyr Client OS，QEMU平台为例，讲解混合部署Client OS。
https://github.com/zephyrproject-rtos/zephyr/tree/main/subsys/ipc/rpmsg_service
https://github.com/vonhust/zephyr/tree/mcs_rpi/drivers/ipm/ipm_gic_sgi.c

一、Client OS侧：将libmetal、openamp代码移植进Client OS，生成clientos.bin
1、libmetal、openamp使用v2022.04版本
https://github.com/OpenAMP/libmetal/tree/v2022.04
https://github.com/OpenAMP/open-amp/tree/v2022.04

OpenAMP <- IPI <- IPM <- SGI7
2、openamp依赖IPI（核间中断），在Linux侧初始化软中断SGI作为IPI（已完成），Client OS侧不能再重复初始化SGI，Client OS只要完成使能SGI、实现SGI的ISR中断服务程序、实现SGI的中断发送接口的工作。

注意：在ARM中，中断SGI虽然是CPU核私有的，但是openamp要求Linux和Client OS使用同一个IPI，所以Client OS在选用SGI时，还要考虑Linux的SGI占用情况。在Linux中，SGI0-6已被Linux内核占用，SGI8-15已被ARM EL3安全固件占用，所以仅SGI7可用。

为什么openamp要求Linux和Client OS使用同一个IPI？
在GIC中，SGI0-15的中断路由是硬件配置好的，各CPU间的同一个SGI是连接好的。假设有4个CPU，SGI7已初始化好（由Linux初始化），当CPU0发送SGI7时，可以指定到达CPU1-3。如果CPU1使能了SGI7（包括使能中断号7的发送和接收），CPU1就会上报中断号7，即能接收到来自CPU0的SGI7。

（1）Zephyr使用IPM作为IPI，初始化和使能IPM(Inter-process mailbox)
【1】初始化IPM队列，注册中断服务程序
	/* Start IPM workqueue */
	k_work_queue_start(&ipm_work_q, ipm_stack_area,
			   K_THREAD_STACK_SIZEOF(ipm_stack_area),
			   IPM_WORK_QUEUE_PRIORITY, NULL);
	k_thread_name_set(&ipm_work_q.thread, "ipm_work_q");

	/* Setup IPM workqueue item */
	k_work_init(&ipm_work, ipm_callback_process);
		…
		gic_sgi_irq_config_func_0
			IRQ_CONNECT(7, IRQ_DEFAULT_PRIORITY,
gic_sgi_isr, DEVICE_DT_INST_GET(0), 0)   #注册SGI7的中断服务程序
【2】使用SGI7实现IPM，对齐Linux，，使能中断
	ipm_handle = device_get_binding(CONFIG_RPMSG_SERVICE_IPM_NAME);
	ipm_register_callback(ipm_handle, ipm_callback, NULL);
	err = ipm_set_enabled(ipm_handle, 1);
		…
		gic_sgi_irq_config_func_0
			irq_enable(7)   #使能SGI7
（2）使用IPM发送中断
virtio_notify
	ipm_send
		…
		gic_sgi_send
			gic_raise_sgi(7, cpu_id, cpu_mask)   #发送SGI7给Linux
（3）使用IPM接收中断
ipm_callback
	k_work_submit_to_queue(&ipm_work_q, &ipm_work);

ipm_callback_process
	virtqueue_notification(vq[1]);

OpenAMP <- virtio device <- shm <- libmetal配置
3、实现libmetal底层接口
（1）初始化日志接口
	err = metal_init(&metal_params);

（2）初始化virtio device shm共享内存
	err = metal_register_generic_device(&shm_device);

static metal_phys_addr_t shm_physmap[] = { SHM_START_ADDR };
static struct metal_device shm_device = {
	.name = SHM_DEVICE_NAME,
	.bus = NULL,
	.num_regions = 1,
	{
		{
			.virt       = (void *) SHM_START_ADDR,
			.physmap    = shm_physmap,
			.size       = SHM_SIZE,
			.page_shift = DEFAULT_PAGE_SHIFT,
			.page_mask  = DEFAULT_PAGE_MASK,
			.mem_flags  = 0,
			.ops        = { NULL },
		},
	},
	.node = { NULL },
	.irq_num = 0,
	.irq_info = NULL
};

注意：openamp依赖vdev内存，vdev内存包括shm内存和vdev_status内存。openamp要求Linux和Client OS使用同一片共享内存。在QEMU中，vdev内存需选用设备内存类型。以下是Zephyr openamp内存布局，供参考。
vdev
(0x70000000-0x7002ffff)	shm(0x70004000-0x7002ffff)	vring_tx(0x7002c000-0x7002ffff)
		vring_rx(0x70028000-0x7002bfff)
		…
	vdev_status(0x70000000-0x70003fff)

（3）获取shm对应的io空间，供vdev设备使用
	err = metal_device_open("generic", SHM_DEVICE_NAME, &device);
	*io = metal_device_io_region(device, 0);

4、初始化vdev设备，创建endpoint
（1）初始化vdev设备，创建tx\rx virtqueue，实现vdev_status寄存器读写
	/* Virtqueue setup */
	vq[0] = virtqueue_allocate(VRING_SIZE);
	vq[1] = virtqueue_allocate(VRING_SIZE);

	rvrings[0].io = *io;
	rvrings[0].info.vaddr = (void *)VRING_TX_ADDRESS;
	rvrings[0].info.num_descs = VRING_SIZE;
	rvrings[0].info.align = VRING_ALIGNMENT;
	rvrings[0].vq = vq[0];

	rvrings[1].io = *io;
	rvrings[1].info.vaddr = (void *)VRING_RX_ADDRESS;
	rvrings[1].info.num_descs = VRING_SIZE;
	rvrings[1].info.align = VRING_ALIGNMENT;
	rvrings[1].vq = vq[1];

	vdev->role = RPMSG_ROLE;
	vdev->vrings_num = VRING_COUNT;
	vdev->func = &dispatch;
	vdev->vrings_info = &rvrings[0];

	err = rpmsg_init_vdev(&rvdev, &vdev, NULL, io, NULL);
	rdev = rpmsg_virtio_get_rpmsg_device(&rvdev);

virtio_get_status
	return sys_read8(VDEV_STATUS_ADDR);

virtio_set_status
	sys_write8(status, VDEV_STATUS_ADDR);

（2）创建endpoint，注册消息接收回调函数
			err = rpmsg_create_ept(&endpoints[i].ep,
						rdev,
						endpoints[i].name,
						RPMSG_ADDR_ANY,
						RPMSG_ADDR_ANY,
						uart_rpmsg_endpoint_cb,
						rpmsg_service_unbind);

5、编写简单的通信程序，与Linux通信
（1）发送消息
rpmsg_service_send
	return rpmsg_send(&endpoints[endpoint_id].ep, data, len);

（2）接收消息
static int uart_rpmsg_endpoint_cb(struct rpmsg_endpoint *ept, void *data,
		size_t len, uint32_t src, void *priv)
{
	/* TODO: receive messages */
}


二、Linux侧：加载clientos.bin，Linux与Client OS开始通信
1、构建openeuler-image镜像，安装sdk，编译出rpmsg_main和mcs_km.ko，步骤参考下面README.md。
https://gitee.com/openeuler/yocto-embedded-tools/tree/master/mcs

2、启动QEMU，-m 1G指定物理内存大小，内核启动参数mem=768M指定系统内存大小，剩下256M（0x70000000-0x7fffffff）作为保留的设备内存，供openamp和加载clientos.bin使用。以下是QEMU内存布局，供参考。
reserved device memory(0x70000000-0x7fffffff)	clientos.bin(0x7a000000-0x7fffffff)
	…
	vdev(0x70000000-0x7002ffff)
system_ram(0x40000000-0x6fffffff)

3、执行rpmsg_main
./rpmsg_main -c [cpu_id] -t [target_binfile] -a [target_binaddress]

cpu_id表示Client OS运行在哪个核上，比如运行在4核中的CPU3，则-c 3。
target_binfile表示Client OS的bin file名字，比如叫做clientos.bin，则-t clientos.bin。
target_binaddress表示Client OS从哪个地址开始运行，比如从0x7a000000开始运行，则-a 0x7a000000。在ARM64中，clientos.bin的加载地址就是运行地址。

例子：./rpmsg_main -c 3 -t clientos.bin -a 0x7a000000

